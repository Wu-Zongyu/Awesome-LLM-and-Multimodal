# LLM-and-VLM-Paper-List
A paper list about large language models and multi-modal models. **Note:** It only records papers for personal needs. It is welcome to open an issue if you think I missed some important and exciting work

## Table of Contents

- [Survey](#survey)
- [LLM](llm)
  - [Foundation LLM Models](#foundation-llm-models)
  - [Reinforcement Learning from Human Feedback (RLHF)](#rlhf)
  - [Parameter Efficient Fine-tuning](#parameter-efficient-fine-tuning)
  - [Healthcare LLM](#healthcare-llm)
  - [Watermarking LLM](#watermarking-llm)
- [Multi-Modal Models](#multi-modal-models)
  - [Foundation Multi-Modal Models](#foundation-multi-modal-models)
- [Useful Resource](#useful-resource)

## Survey
- HELM: **Holistic evaluation of language models**. Arxiv'2022. [paper](https://arxiv.org/abs/2211.09110)
- Eval Survey: **A Survey on Evaluation of Large Language Models**. Arxiv'2023. [paper](https://arxiv.org/abs/2307.03109)
- Healthcare LM Survey: **A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics**. Arxiv'2023. [paper](https://arxiv.org/abs/2310.05694)
- VLM for vision Task Survey: **Vision Language Models for Vision Tasks: A Survey**. Arxiv'2023. [paper](https://arxiv.org/abs/2304.00685)
---

## LLM
### Foundation LLM Models
- Transformer: **Attention Is All You Need**. NIPS'2017. [paper](https://arxiv.org/abs/1706.03762)
- GPT-1: **Improving Language Understanding by Generative Pre-Training**. 2018. [paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
- BERT: **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**. NAACL'2019. [paper](https://aclanthology.org/N19-1423.pdf)
- GPT-2: **Language Models are Unsupervised Multitask Learners**. 2018. [paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- RoBERTa: **RoBERTa: A Robustly Optimized BERT Pretraining Approach**. Arxiv'2019, [paper](https://arxiv.org/abs/1907.11692)
- DistilBERT: **DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter**. Arxiv'2019. [paper](https://arxiv.org/abs/1910.01108)
- T5: **Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer**. JMLR'2020. [paper](https://arxiv.org/abs/1910.10683)
- GPT-3: **Language Models are Few-Shot Learners**. NeurIPS'2020. [paper](https://arxiv.org/abs/2005.14165)
- PaLM: **PaLM: Scaling Language Modeling with Pathways**. ArXiv'2022. [paper](https://arxiv.org/abs/2204.02311)
- BLOOM:  **BLOOM: A 176B-Parameter Open-Access Multilingual Language Model**. Arxiv'2022. [paper](https://arxiv.org/abs/2211.05100)
- GLaM: **GLaM: Efficient Scaling of Language Models with Mixture-of-Experts**. ICML'2022. [paper](https://arxiv.org/abs/2112.06905)
- LLaMA: **LLaMA: Open and Efficient Foundation Language Models**. Arxiv'2023. [paper](https://arxiv.org/abs/2302.13971)
- GPT-4: **GPT-4 Technical Report**. Arxiv'2023. [paper]([http://arxiv.org/abs/2303.08774v2](https://arxiv.org/abs/2303.08774v4))
- PaLM 2: **PaLM 2 Technical Report**. 2023. [paper](https://arxiv.org/abs/2305.10403)
- LLaMA 2: **Llama 2: Open foundation and fine-tuned chat models**. Arxiv'2023. [paper](https://arxiv.org/abs/2307.09288)
- Mistral: **Mistral 7B**. Arxiv'2023. [paper](https://arxiv.org/abs/2310.06825)
- Phi1: [link](https://huggingface.co/microsoft/phi-1)
- Phi1.5: [Project Link](https://huggingface.co/microsoft/phi-1_5)
- Phi2: [Project Link](https://huggingface.co/microsoft/phi-2)
- Falcon: [Project Link](https://huggingface.co/tiiuae)
### RLHF 
- PPO: **Proximal Policy Optimization Algorithms**. Arxiv'2017. [paper](https://arxiv.org/abs/1707.06347)
- DPO: **Direct Preference Optimization: Your Language Model is Secretly a Reward Model**. NeurIPS'2023. [paper](https://arxiv.org/abs/2305.18290)
### Parameter Efficient Fine-tuning
- LoRA: **LoRA: Low-Rank Adaptation of Large Language Models**. Arxiv'2021. [paper](https://arxiv.org/abs/2106.09685)
- Prefix-Tuning: **Prefix-Tuning: Optimizing Continuous Prompts for Generation**. ACL'2021. [paper](https://aclanthology.org/2021.acl-long.353/)
- P-tuning: **P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks**. ACL'2022. [paper](https://aclanthology.org/2022.acl-short.8/)
- P-tuning v2: **P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks**. Arxiv'2022. [Paper](https://arxiv.org/abs/2110.07602)
- Q-LoRA: **QLoRA: Efficient Finetuning of Quantized LLMs**. NeurIPS'2023. [paper](https://arxiv.org/abs/2305.14314)
### Healthcare LLM
- 
### Watermarking LLM
---

## Multi-modal Models
### Foundation Multi-Modal Models
- CLIP: **Learning Transferable Visual Models From Natural Language Supervision**. ICML'2021. [paper](https://arxiv.org/abs/2103.00020)
- DeCLIP: **Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm**. ICLR'2022. [paper](https://arxiv.org/abs/2110.05208)
- FILIP: **FILIP: Fine-grained Interactive Language-Image Pre-Training**. ICLR'2022. [paper](https://arxiv.org/abs/2111.07783)
- Stable Diffusion: **High-Resolution Image Synthesis with Latent Diffusion Models**. CVPR'2022. [paper](https://arxiv.org/abs/2112.10752)
- LLaVA: **Visual Instruction Tuning**. NeurIPS'2022. [paper](https://arxiv.org/abs/2304.08485)
- BLIP: **BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation**. ICML'2022. [paper](https://arxiv.org/abs/2201.12086)
- BLIP2: **BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models**. ICML'2023. [paper](https://arxiv.org/abs/2301.12597)

---

## Useful-Resource
- LLaMA Factory. https://github.com/hiyouga/LLaMA-Factory
- DeepSpeed. https://github.com/microsoft/DeepSpeed
- trlx. https://github.com/CarperAI/trlx

