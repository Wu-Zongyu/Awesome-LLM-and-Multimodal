# LLM-and-VLM-Paper-List
A paper list about large language models and vision-language models. **Note:** It only records papers for personal needs. It is welcome to open an issue if you think I missed some important and exciting work

## Table of Contents

- [Survey](#survey)
- [LLM](llm)
  - [Foundation LLM Models](#foundation-llm-models)
  - [Reinforcement Learning from Human Feedback (RLHF)](#rlhf)
  - [Parameter Efficient Fine-tuning](#parameter-efficient-fine-tuning)
  - [Medical LLM](#medical-llm)
  - [Watermarking LLM](#watermarking-llm)
- [VLM](#vlm)
- [Useful Resource](#useful-resource)

## Survey
- HELM: **Holistic evaluation of language models**, Arxiv 2022. [paper](https://arxiv.org/pdf/2211.09110.pdf)
---

## LLM
### Foundation LLM Models
- Transformer: **Attention Is All You Need**. NIPS'2017. [paper](https://arxiv.org/abs/1706.03762)
- GPT-1 **Improving Language Understanding by Generative Pre-Training**. 2018. [paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
- BERT: **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**. NAACL'2019. [paper](https://aclanthology.org/N19-1423.pdf)
- GPT-2 **Language Models are Unsupervised Multitask Learners**. 2018. [paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- RoBERTa: **RoBERTa: A Robustly Optimized BERT Pretraining Approach**. Arxiv'2019, [paper](https://arxiv.org/abs/1907.11692)
- DistilBERT: **DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter**. Arxiv'2019. [paper](https://arxiv.org/abs/1910.01108)
- T5: **Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer**. JMLR'2020. [paper](https://arxiv.org/abs/1910.10683)
- GPT-3 **Language Models are Few-Shot Learners**. NeurIPS'2020. [paper](https://arxiv.org/abs/2005.14165)
- PaLM **PaLM: Scaling Language Modeling with Pathways**. ArXiv'2022. [paper](https://arxiv.org/abs/2204.02311)
- BLOOM  **BLOOM: A 176B-Parameter Open-Access Multilingual Language Model**. Arxiv'2022. [paper](https://arxiv.org/abs/2211.05100)
- GLaM **GLaM: Efficient Scaling of Language Models with Mixture-of-Experts**. ICML'2022. [paper](https://arxiv.org/abs/2112.06905)
- LLaMA **LLaMA: Open and Efficient Foundation Language Models**. Arxiv'2023. [paper](https://arxiv.org/abs/2302.13971)
- GPT-4 **GPT-4 Technical Report**. Arxiv'2023. [paper]([http://arxiv.org/abs/2303.08774v2](https://arxiv.org/abs/2303.08774v4))
- PaLM 2: **PaLM 2 Technical Report**. 2023. [paper](https://arxiv.org/abs/2305.10403)
- LLaMA 2: **Llama 2: Open foundation and fine-tuned chat models**. Arxiv'2023. [paper](https://arxiv.org/abs/2307.09288)
### RLHF 
- PPO: **Proximal Policy Optimization Algorithms**. Arxiv'2017. [paper](https://arxiv.org/abs/1707.06347)
- DPO: **Direct Preference Optimization: Your Language Model is Secretly a Reward Model**. NeurIPS'2023. [paper](https://arxiv.org/abs/2305.18290)
### Parameter Efficient Fine-tuning
- LoRA: **LoRA: Low-Rank Adaptation of Large Language Models**. Arxiv'2021. [paper](https://arxiv.org/abs/2106.09685)
- Prefix-Tuning: **Prefix-Tuning: Optimizing Continuous Prompts for Generation**. ACL'2021. [paper](https://aclanthology.org/2021.acl-long.353/)
- P-tuning: **P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks**. ACL'2022. [paper](https://aclanthology.org/2022.acl-short.8/)
- P-tuning v2: **P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks**. Arxiv'2022. [Paper](https://arxiv.org/abs/2110.07602)
- Q-LoRA: **QLoRA: Efficient Finetuning of Quantized LLMs**. NeurIPS'2023. [paper](https://arxiv.org/abs/2305.14314)
### Medical LLM
### Watermarking LLM
---

## VLM

---

## Useful-Resource
- LLaMA Factory. https://github.com/hiyouga/LLaMA-Factory
- DeepSpeed. https://github.com/microsoft/DeepSpeed
- trlx. https://github.com/CarperAI/trlx

